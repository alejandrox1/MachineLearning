{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Function approximation\n",
    "\n",
    "* Inputs == Predictors == Independent variables == features\n",
    "* Outputs == responses == Dependent variables\n",
    "\n",
    "\n",
    "* The last column of a Dataset always corresponds to the class label"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from csv import reader\n",
    "\n",
    "def load_csv(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        lines = reader(file)\n",
    "        dataset = list(lines)\n",
    "    dataset = [ row for row in dataset if row ]\n",
    "    return dataset\n",
    "    \n",
    "def str_column_to_float(dataset, column):     \n",
    "    \"\"\" Convert string numerical entries to floats. \"\"\"\n",
    "    for row in dataset:                                                     \n",
    "        row[column] = float(row[column].strip())\n",
    "                                         \n",
    "def str_column_to_int(dataset, column):    \n",
    "    \"\"\" Convert string label entries to ints. \"\"\"\n",
    "    class_values = [row[column] for row in dataset]                         \n",
    "    unique = set(class_values)                                              \n",
    "    lookup = dict()                                                         \n",
    "    for i, value in enumerate(unique):                                      \n",
    "        lookup[value] = i                                               \n",
    "    for row in dataset:                                                     \n",
    "        row[column] = lookup[row[column]]                               \n",
    "    return lookup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file ../datasets/pima-indians-diabetes.csv with 768 rows and 9 columns\n",
      "['6', '148', '72', '35', '0', '33.6', '0.627', '50', '1']\n",
      "[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0, 1.0]\n",
      "\n",
      "\n",
      "Loaded data file ../datasets/iris.csv with 150 rows and 5 columns\n",
      "['5.1', '3.5', '1.4', '0.2', 'Iris-setosa']\n",
      "[5.1, 3.5, 1.4, 0.2, 'Iris-setosa']\n",
      "{'Iris-virginica': 0, 'Iris-versicolor': 1, 'Iris-setosa': 2}\n"
     ]
    }
   ],
   "source": [
    "filename = '../datasets/pima-indians-diabetes.csv'             \n",
    "dataset = load_csv(filename)\n",
    "print('Loaded data file {0} with {1} rows and {2} columns'.format(filename, len(dataset), len(dataset[0])))\n",
    "\n",
    "print(dataset[0])\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i) \n",
    "print(dataset[0])\n",
    "\n",
    "\n",
    "\n",
    "filename = '../datasets/iris.csv' \n",
    "dataset = load_csv(filename)\n",
    "print('\\n\\nLoaded data file {0} with {1} rows and {2} columns'.format(filename, len(dataset), len(dataset[0])))\n",
    "\n",
    "print(dataset[0])  \n",
    "for i in range(4):               \n",
    "    str_column_to_float(dataset, i) \n",
    "print(dataset[0])\n",
    "\n",
    "lookup = str_column_to_int(dataset, 4)  \n",
    "print(lookup)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Scaling\n",
    "\n",
    "Scale of input and output to be equivalent.\n",
    "\n",
    "Standardization assumes your data conforms to a normal distribution.\n",
    "Normalization is more sensitive to outliers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dataset_minmax(dataset):\n",
    "    minmax = []\n",
    "    for i in range(len(dataset[0])): # for each feature\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        value_min = min(col_values)\n",
    "        value_max = max(col_values)\n",
    "        minmax.append( [value_min, value_max] )\n",
    "    return minmax\n",
    "\n",
    "def normalize_dataset(dataset):\n",
    "    \"\"\" Scaled value = value - min / max - min \"\"\"\n",
    "    minmax = dataset_minmax(dataset)\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i]-minmax[i][0]) / (minmax[i][1]-minmax[i][0])\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file ../datasets/pima-indians-diabetes.csv with 768 rows and 9 columns\n",
      "[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0, 1.0]\n",
      "[0.35294117647058826, 0.7437185929648241, 0.5901639344262295, 0.35353535353535354, 0.0, 0.5007451564828614, 0.23441502988898377, 0.48333333333333334, 1.0]\n"
     ]
    }
   ],
   "source": [
    "# normalize \n",
    "filename = '../datasets/pima-indians-diabetes.csv'             \n",
    "dataset = load_csv(filename)\n",
    "print('Loaded data file {0} with {1} rows and {2} columns'.format(filename, len(dataset), len(dataset[0])))\n",
    "# convert strings to float\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i) \n",
    "print(dataset[0])\n",
    "dataset = normalize_dataset(dataset)                                              \n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from math import sqrt\n",
    "\n",
    "def column_means(dataset):\n",
    "    means = [0 for i in range(len(dataset[0]))]\n",
    "    for i in range(len(dataset[0])):\n",
    "        col_values = [row[i] for row in dataset]\n",
    "        means[i] = sum(col_values) / float(len(dataset))\n",
    "    return means\n",
    "\n",
    "def column_stdevs(dataset):\n",
    "    \"\"\" var = sum( value - mean )**2 / . \"\"\"\n",
    "    means = column_means(dataset)\n",
    "    stdevs = [0 for i in range(len(dataset[0]))]\n",
    "    for i in range(len(dataset[0])):\n",
    "        valriance = [ (row[i]-means[i])**2.0 for row in dataset ]\n",
    "        stdevs[i] = sum(valriance)\n",
    "    stdevs = [ sqrt(x/float(len(dataset)-1)) for x in stdevs ]\n",
    "    return stdevs, means\n",
    "\n",
    "def standardize_dataset(dataset):\n",
    "    stdevs, means = column_stdevs(dataset)\n",
    "    for row in dataset:\n",
    "        for i in range(len(row)):\n",
    "            row[i] = (row[i] - means[i]) / stdevs[i]\n",
    "    return dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded data file ../datasets/pima-indians-diabetes.csv with 768 rows and 9 columns\n",
      "[6.0, 148.0, 72.0, 35.0, 0.0, 33.6, 0.627, 50.0, 1.0]\n",
      "[0.6395304921176576, 0.8477713205896718, 0.14954329852954296, 0.9066790623472505, -0.692439324724129, 0.2038799072674717, 0.468186870229798, 1.4250667195933604, 1.3650063669598067]\n"
     ]
    }
   ],
   "source": [
    "# standardize\n",
    "filename = '../datasets/pima-indians-diabetes.csv'             \n",
    "dataset = load_csv(filename)\n",
    "print('Loaded data file {0} with {1} rows and {2} columns'.format(filename, len(dataset), len(dataset[0])))\n",
    "# convert strings to float\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i) \n",
    "print(dataset[0])                                                                                                      \n",
    "# standardize dataset                                                           \n",
    "dataset = standardize_dataset(dataset)                                     \n",
    "print(dataset[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resampling Methods\n",
    "\n",
    "If multiple algorithms are compared the same train test split should be used for consistent comparison.\n",
    "\n",
    "$k$-fold cross validation helps reduce noise of performance estimates.\n",
    "Here, the algorithm is trained and evaluated $k$ times and the performance is summarized by taking the mean of the performance score.\n",
    "\n",
    "Train on $k$-1 folds and evaluate on the $k$th one.\n",
    "Then repeat so that each of the $k$ groups is given an opportunity to be used as a test set.\n",
    "\n",
    "**A quick way to check if the fold sizes are representative is to calculate summary statistics (i.e., mean and standard deviation) and see how much the values differ from the statistics of the entire set.**\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "from time import time\n",
    "from random import seed\n",
    "from random import randrange\n",
    "\n",
    "def train_test_split(dataset, split=0.60):\n",
    "    seed( os.getpid() * time())\n",
    "    train = []\n",
    "    train_size = split * len(dataset)\n",
    "    dataset_copy = list(dataset)\n",
    "    while len(train) < train_size:\n",
    "        index = randrange(len(dataset_copy))\n",
    "        train.append( dataset_copy.pop(index) )\n",
    "    return train, dataset_copy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7], [1], [8], [3], [10], [5]]\n",
      "[[2], [4], [6], [9]]\n"
     ]
    }
   ],
   "source": [
    "dataset = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]                   \n",
    "train, test = train_test_split(dataset)                                         \n",
    "print(train)                                                                    \n",
    "print(test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def cross_validation_split(dataset, n_folds=3):\n",
    "    seed( os.getpid() * time())\n",
    "    dataset_split = []\n",
    "    dataset_copy = list(dataset)\n",
    "    fold_size = len(dataset) // n_folds\n",
    "    for i in range(n_folds):\n",
    "        fold = []\n",
    "        while len(fold) < fold_size:\n",
    "            index = randrange(len(dataset_copy))\n",
    "            fold.append( dataset_copy.pop(index) )\n",
    "        dataset_split.append( fold )\n",
    "    return dataset_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[4], [1], [6]]\n",
      "[[2], [10], [8]]\n",
      "[[5], [3], [9]]\n"
     ]
    }
   ],
   "source": [
    "dataset = [[1], [2], [3], [4], [5], [6], [7], [8], [9], [10]]                   \n",
    "folds = cross_validation_split(dataset)                                      \n",
    "for fold in folds:\n",
    "    print(fold)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Evaluation Metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def accuracy_metric(actual, predicted):\n",
    "    correct = 0\n",
    "    for i in range(len(actual)):\n",
    "        if actual[i] == predicted[i]:\n",
    "            correct += 1\n",
    "    return correct / float(len(actual))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8\n"
     ]
    }
   ],
   "source": [
    "# Test accuracy\n",
    "actual    = [0,0,0,0,0,1,1,1,1,1]\n",
    "predicted = [0,1,0,0,0,1,0,1,1,1]\n",
    "accuracy = accuracy_metric(actual, predicted)\n",
    "print(accuracy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def confusion_matrix(actual, predicted):\n",
    "    unique = list(set(actual))\n",
    "    matrix = [[] for i in range(len(unique))]\n",
    "    for i in range(len(unique)):\n",
    "        matrix[i] = [0 for i in range(len(unique))]\n",
    "        \n",
    "    lookup = dict()\n",
    "    for i, value in enumerate(unique): # assign id to each label\n",
    "        lookup[value] = i\n",
    "    for i in range(len(actual)):\n",
    "        x = lookup[actual[i]]\n",
    "        y = lookup[predicted[i]]\n",
    "        matrix[x][y] += 1\n",
    "    return unique, matrix\n",
    "\n",
    "def print_confusion_matrix(actual, predicted):\n",
    "    unique, matrix = confusion_matrix(actual, predicted)\n",
    "    print(' '*3, end='')\n",
    "    for i in unique:\n",
    "        print(i, end='  ')\n",
    "    print()\n",
    "    for i in range(len(matrix)):\n",
    "        print(unique[i], matrix[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   0  1  \n",
      "0 [3, 2]\n",
      "1 [1, 4]\n"
     ]
    }
   ],
   "source": [
    "# Test confusion matrix with integers\n",
    "actual    = [0,0,0,0,0,1,1,1,1,1]\n",
    "predicted = [0,1,1,0,0,1,0,1,1,1]\n",
    "print_confusion_matrix(actual, predicted)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def mae_metric(actual, predicted):    \n",
    "    \"\"\" Mean Absolute Error\"\"\"\n",
    "    sum_error = 0.0                                                         \n",
    "    for i in range(len(actual)):                                            \n",
    "        sum_error += abs(predicted[i] - actual[i])                      \n",
    "    return sum_error / float(len(actual))\n",
    "\n",
    "def rmse_metric(actual, predicted):                                             \n",
    "    sum_error = 0.0                                                         \n",
    "    for i in range(len(actual)):                                            \n",
    "        prediction_error = predicted[i] - actual[i]                     \n",
    "        sum_error += (prediction_error ** 2)                            \n",
    "    mean_error = sum_error / float(len(actual))                             \n",
    "    return sqrt(mean_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Baselines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0]\n"
     ]
    }
   ],
   "source": [
    "def random_algorithm(train, test):                                              \n",
    "    output_values = [row[-1] for row in train]                              \n",
    "    unique = list(set(output_values))                                       \n",
    "    predicted = []                                           \n",
    "    for row in test:                                                        \n",
    "        index = randrange(len(unique))                                  \n",
    "        predicted.append(unique[index])                                 \n",
    "    return predicted                                                        \n",
    "                                                                                \n",
    "seed(1)                                                                         \n",
    "train = [[0], [1], [0], [1], [0], [1]]                                          \n",
    "test = [[None], [None], [None], [None]]                                         \n",
    "predictions = random_algorithm(train, test)                                     \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['0', '0', '0', '0', '0', '0']\n"
     ]
    }
   ],
   "source": [
    "def zero_rule_algorithm_classification(train, test):                            \n",
    "    output_values = [row[-1] for row in train]                              \n",
    "    prediction = max(set(output_values), key=output_values.count)           \n",
    "    predicted = [prediction for i in range(len(train))]                     \n",
    "    return predicted                                                        \n",
    "                                                                                \n",
    "seed(1)                                                                         \n",
    "train = [['0'], ['0'], ['0'], ['0'], ['1'], ['1']]                              \n",
    "test = [[None], [None], [None], [None]]                                         \n",
    "predictions = zero_rule_algorithm_classification(train, test)                   \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[15.0, 15.0, 15.0, 15.0]\n"
     ]
    }
   ],
   "source": [
    "def zero_rule_algorithm_regression(train, test):                                \n",
    "    output_values = [row[-1] for row in train]                              \n",
    "    prediction = sum(output_values) / float(len(output_values))             \n",
    "    predicted = [prediction for i in range(len(test))]                      \n",
    "    return predicted                                                        \n",
    "                                                                                \n",
    "seed(1)                                                                         \n",
    "train = [[10], [15], [12], [15], [18], [20]]                                    \n",
    "test = [[None], [None], [None], [None]]                                         \n",
    "predictions = zero_rule_algorithm_regression(train, test)                       \n",
    "print(predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test Harness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train Test Split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_algorithm_ttsplit(dataset, algorithm, split, *args):    \n",
    "    # Train Test Split\n",
    "    train, test = train_test_split(dataset, split)                          \n",
    "    test_set = []                                                      \n",
    "    for row in test:                                                        \n",
    "        row_copy = list(row)     \n",
    "        # delete class label\n",
    "        row_copy[-1] = None                                             \n",
    "        test_set.append(row_copy)\n",
    "    # Fit\n",
    "    predicted = algorithm(train, test_set, *args)                           \n",
    "    actual = [row[-1] for row in test] \n",
    "    # Measure\n",
    "    accuracy = accuracy_metric(actual, predicted)                           \n",
    "    return accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.6547231270358306\n"
     ]
    }
   ],
   "source": [
    "filename = '../datasets/pima-indians-diabetes.csv'             \n",
    "dataset = load_csv(filename)\n",
    "# convert strings to float\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i) \n",
    "\n",
    "# evaluate algorithm                                                            \n",
    "split = 0.6                                                                     \n",
    "accuracy = evaluate_algorithm_ttsplit(dataset, zero_rule_algorithm_classification, split)\n",
    "print('Accuracy: {}'.format(accuracy))    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## K-fold learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def evaluate_algorithm_kfold(dataset, algorithm, n_folds, *args):  \n",
    "    # K folds\n",
    "    folds = cross_validation_split(dataset, n_folds)                        \n",
    "    scores = []                                                       \n",
    "    for fold in folds:                                                      \n",
    "        train_set = list(folds)                                         \n",
    "        train_set.remove(fold)                                          \n",
    "        train_set = sum(train_set, [])                                  \n",
    "        test_set = list()\n",
    "        # prep test set for each iteration\n",
    "        for row in fold:                                                \n",
    "            row_copy = list(row)                                    \n",
    "            test_set.append(row_copy)                               \n",
    "            row_copy[-1] = None           \n",
    "        # fit\n",
    "        predicted = algorithm(train_set, test_set, *args)               \n",
    "        actual = [row[-1] for row in fold]                              \n",
    "        accuracy = accuracy_metric(actual, predicted)                   \n",
    "        scores.append(accuracy)                                         \n",
    "    return scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: [0.6797385620915033, 0.6339869281045751, 0.6666666666666666, 0.6209150326797386, 0.6470588235294118]\n"
     ]
    }
   ],
   "source": [
    "filename = '../datasets/pima-indians-diabetes.csv'             \n",
    "dataset = load_csv(filename)\n",
    "# convert strings to float\n",
    "for i in range(len(dataset[0])):\n",
    "    str_column_to_float(dataset, i) \n",
    "\n",
    "# evaluate algorithm                                                            \n",
    "n_folds = 5\n",
    "accuracy = evaluate_algorithm_kfold(dataset, zero_rule_algorithm_classification, n_folds)\n",
    "print('Accuracy: {}'.format(accuracy))  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Save ypour work"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[NbConvertApp] Converting notebook Getting_Started.ipynb to script\n",
      "[NbConvertApp] Writing 14501 bytes to Getting_Started.py\n"
     ]
    }
   ],
   "source": [
    "!jupyter nbconvert --to script Getting_Started.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
